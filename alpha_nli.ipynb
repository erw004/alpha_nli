{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm83di9rV3UP",
        "outputId": "8cefb181-aba6-4c12-cd0e-7e305060641b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m160.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.optim import Optimizer, AdamW, Adam, SGD, RMSprop\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoModelForSequenceClassification, RobertaForMultipleChoice"
      ],
      "metadata": {
        "id": "cHD0xTPOaLW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AlphaNLIExample:\n",
        "    choices: list[str]\n",
        "    label: int\n",
        "\n",
        "    @staticmethod\n",
        "    def from_dict(data: dict):\n",
        "      return AlphaNLIExample(\n",
        "        choices=data[\"choices\"],\n",
        "        label=data[\"label\"],\n",
        "      )\n",
        "\n",
        "\n",
        "def initialize_datasets(tokenizer: PreTrainedTokenizerFast, sample: bool, data_file: str, label_data: str, sample_size: int = 0) -> dict:\n",
        "    # load dataset\n",
        "    raw_data = load_dataset(\"json\", data_files=data_file)\n",
        "\n",
        "    # read labels\n",
        "    labels = list()\n",
        "    with open(label_data, \"r\") as f:\n",
        "       labels = [int(line.strip())-1 for line in f]\n",
        "\n",
        "    # add labels to dataset\n",
        "    raw_data[\"train\"] = raw_data[\"train\"].add_column(\"label\", labels)\n",
        "\n",
        "    # generate our choices (based on linear chain: obs1 -> hyp -> obs2)\n",
        "    raw_data[\"train\"] = raw_data[\"train\"].add_column(\"choices\", [[x[\"obs1\"] + \" \" + x[\"hyp1\"] + \" \" + x[\"obs2\"],\n",
        "                                                                x[\"obs1\"] + \" \" + x[\"hyp2\"] + \" \" + x[\"obs2\"]] for x in raw_data[\"train\"]])\n",
        "\n",
        "    # just for now - take random sampling of 1000\n",
        "    #dataset = raw_data\n",
        "    dataset = {\n",
        "        \"train\": raw_data[\"train\"].shuffle(seed=42).select(range(sample_size)) if sample else raw_data[\"train\"]\n",
        "    }\n",
        "\n",
        "    # initialize as AlphaNLI Dataset\n",
        "    split_datasets = {}\n",
        "    for split_name in dataset.keys():\n",
        "      split_data = list(dataset[split_name])\n",
        "      split_datasets[split_name] = AlphaNLIDataset(tokenizer, split_data)\n",
        "\n",
        "    return split_datasets"
      ],
      "metadata": {
        "id": "-u-qgHGR2MOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaNLIDataset(Dataset):\n",
        "    tokenizer: PreTrainedTokenizerFast = None\n",
        "\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizerFast, raw_data_list: List[dict]):\n",
        "        AlphaNLIDataset.tokenizer = tokenizer\n",
        "        self.sample_list = [AlphaNLIExample.from_dict(data) for data in raw_data_list]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sample_list[idx]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.sample_list)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batched_samples: List[AlphaNLIExample]) -> dict:\n",
        "        batched_choices = [sample.choices for sample in batched_samples]\n",
        "        batched_label = [sample.label for sample in batched_samples]\n",
        "\n",
        "        # choice_encoding = AlphaNLIDataset.tokenizer(batched_choices,\n",
        "        #                                       padding=True,\n",
        "        #                                       max_length=512,\n",
        "        #                                       truncation=True,\n",
        "        #                                       return_tensors=\"pt\")\n",
        "        tokenized_choices = []\n",
        "        for choices in batched_choices:\n",
        "            # Tokenize each choice in the pair\n",
        "            tokenized_pair = AlphaNLIDataset.tokenizer(\n",
        "                choices,  # List of strings (e.g., [\"obs1 hyp1 obs2\", \"obs1 hyp2 obs2\"])\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=256,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            tokenized_choices.append(tokenized_pair)\n",
        "\n",
        "        # Stack tokenized inputs into the correct shape\n",
        "        input_ids = torch.stack([tc[\"input_ids\"] for tc in tokenized_choices])  # (batch_size, num_choices, seq_len)\n",
        "        attention_mask = torch.stack([tc[\"attention_mask\"] for tc in tokenized_choices])\n",
        "\n",
        "        label_encoding = torch.LongTensor(batched_label)\n",
        "\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': label_encoding}"
      ],
      "metadata": {
        "id": "UmVjzxII_0JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is like ALL AI - so should change this up a little\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    optimizer: Optimizer,\n",
        "    epoch: int,\n",
        "    gradient_accumulation_steps: int = 4,  # Number of steps to accumulate gradients\n",
        "    target_batch_size: int = 32,  # Simulated larger batch size\n",
        "):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    effective_batch_size = target_batch_size\n",
        "    actual_batch_size = dataloader.batch_size\n",
        "    assert effective_batch_size % actual_batch_size == 0, (\n",
        "        f\"Target batch size ({effective_batch_size}) must be divisible by \"\n",
        "        f\"actual batch size ({actual_batch_size}).\"\n",
        "    )\n",
        "    gradient_accumulation_steps = effective_batch_size // actual_batch_size\n",
        "\n",
        "    with tqdm(dataloader, desc=f\"Train Ep {epoch}\", total=len(dataloader)) as tq:\n",
        "        for step, batch in enumerate(tq):\n",
        "            input_ids = batch['input_ids'].to(model.device)\n",
        "            attention_mask = batch['attention_mask'].to(model.device)\n",
        "            label_encoding = batch['label'].to(model.device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=label_encoding)\n",
        "            loss = outputs.loss / gradient_accumulation_steps  # Scale loss by accumulation steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Perform optimizer step and zero gradients after accumulating enough steps\n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Update progress bar\n",
        "            tq.set_postfix({\"loss\": loss.detach().item() * gradient_accumulation_steps})  # Rescale loss for display\n",
        "\n",
        "        # Handle the last batch if it doesn't align with gradient_accumulation_steps\n",
        "        if (step + 1) % gradient_accumulation_steps != 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "_Vd-G-oUDL5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, dataloader: DataLoader) -> float:\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with tqdm(dataloader, desc=f\"Eval\", total=len(dataloader)) as tq:\n",
        "        for batch in tq:\n",
        "            with torch.no_grad():\n",
        "                input_ids = batch['input_ids'].to(model.device)\n",
        "                attention_mask = batch['attention_mask'].to(model.device)\n",
        "                label_encoding = batch['label'].to(model.device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=label_encoding)\n",
        "                logits = outputs.logits\n",
        "\n",
        "                predictions = torch.argmax(logits, dim=-1)\n",
        "                labels = label_encoding\n",
        "\n",
        "                all_predictions += predictions\n",
        "                all_labels += labels\n",
        "\n",
        "    all_predictions = torch.Tensor(all_predictions)\n",
        "    all_labels = torch.Tensor(all_labels)\n",
        "    accuracy = compute_accuracy(all_predictions, all_labels)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions: torch.Tensor, labels: torch.Tensor) -> float:\n",
        "    assert predictions.size(-1) == labels.size(-1)\n",
        "    accuracy = (predictions == labels).sum().item() / len(labels)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "CQFeSNL_dgv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(64)\n",
        "\n",
        "def main(batch_size, learning_rate, num_epochs, grad_accum):\n",
        "    model_name = \"distilroberta-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = RobertaForMultipleChoice.from_pretrained(model_name)\n",
        "    model = model.cuda()\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
        "\n",
        "    datasets = initialize_datasets(tokenizer, sample=True, data_file=\"train.jsonl\", label_data=\"train-labels.lst\", sample_size=10000)\n",
        "    print(datasets['train'][0])\n",
        "\n",
        "    train_dataloader = DataLoader(datasets['train'],\n",
        "                                   batch_size=batch_size,\n",
        "                                   shuffle=True,\n",
        "                                   collate_fn=AlphaNLIDataset.collate_fn,\n",
        "                                   num_workers=2)\n",
        "\n",
        "    dev_datasets = initialize_datasets(tokenizer, sample=False, data_file=\"dev.jsonl\", label_data=\"dev-labels.lst\")\n",
        "    dev_dataloader = DataLoader(dev_datasets['train'],\n",
        "                                   batch_size=batch_size,\n",
        "                                   shuffle=True,\n",
        "                                   collate_fn=AlphaNLIDataset.collate_fn,\n",
        "                                   num_workers=2)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_one_epoch(model, train_dataloader, optimizer, epoch, gradient_accumulation_steps=grad_accum, target_batch_size=batch_size)\n",
        "        valid_acc = evaluate(model, dev_dataloader)\n",
        "        best_acc = max(best_acc, valid_acc)\n",
        "    return best_acc"
      ],
      "metadata": {
        "id": "WA6sf-J3PUTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(batch_size=32, learning_rate=5e-5, num_epochs=5, grad_accum=4)"
      ],
      "metadata": {
        "id": "S-pq8abyPt0L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}